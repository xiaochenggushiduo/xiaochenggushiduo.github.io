(window.webpackJsonp=window.webpackJsonp||[]).push([[6],{168:function(_,v,t){"use strict";t.r(v);var i=t(0),s=Object(i.a)({},function(){this.$createElement;this._self._c;return this._m(0)},[function(){var _=this,v=_.$createElement,t=_._self._c||v;return t("div",{staticClass:"content"},[t("h1",{attrs:{id:"深度学习"}},[_._v("深度学习")]),_._v(" "),t("p",[_._v("标准的神经网络（NN）可用于训练房子特征和房价之间的函数，")]),_._v(" "),t("p",[_._v("卷积神经网络（CNN）可用于训练图像和类别之间的函数，")]),_._v(" "),t("p",[_._v("循环神经网络（RNN）可用于训练语音和文本之间的函数")]),_._v(" "),t("p",[_._v("NN 使用的是"),t("strong",[_._v("权重矩阵")]),_._v("（连接）和节点值相乘并陆续传播至下一层节点的方式；")]),_._v(" "),t("p",[_._v("CNN 使用"),t("strong",[_._v("矩形卷积核")]),_._v("在图像输入上依次进行卷积操作、滑动，得到下一层输入的方式；")]),_._v(" "),t("p",[_._v("RNN "),t("strong",[_._v("记忆或遗忘先前时间步的信息")]),_._v("以为当前计算过程提供长期记忆。")]),_._v(" "),t("p",[_._v("深度学习研究的一大突破是新型激活函数的出现，"),t("strong",[_._v("用 ReLU 函数替换sigmoid 函数可以在反向传播中保持快速的梯度下降过程")]),_._v("，sigmoid 函数在正无穷处和负无穷处会出现趋于零的导数，这正是梯度消失导致训练缓慢甚至失败的主要原因。要研究深度学习，需要学会「idea—代码—实验—idea」的良性循环。")]),_._v(" "),t("h2",{attrs:{id:"logistic-回归"}},[_._v("logistic 回归")]),_._v(" "),t("p",[_._v("将 logistic 回归看成将两组数据点分离的问题，如果仅有线性回归（激活函数为线性），则对于非线性边界的数据点（例如，一组数据点被另一组包围）是无法有效分离的，因此在这里需要用非线性激活函数替换线性激活函数。")]),_._v(" "),t("h3",{attrs:{id:"损失函数"}},[_._v("损失函数")]),_._v(" "),t("p",[_._v("这个分类其实就是一个优化问题，优化过程的目的是使预测值 y hat 和真实值 y 之间的差距最小，形式上可以通过寻找目标函数的最小值来实现。所以我们首先确定目标函数（损失函数、代价函数）的形式，然后用梯度下降逐步更新 w、b，当"),t("strong",[_._v("损失函数达到最小值或者足够小")]),_._v("时，我们就能获得很好的预测结果。")]),_._v(" "),t("h3",{attrs:{id:"学习率"}},[_._v("学习率")]),_._v(" "),t("p",[_._v("使用梯度可以找到最快的下降路径，学习率的大小可以决定收敛的速度和最终结果。学习率较大时，初期收敛很快，不易停留在局部极小值，但后期难以收敛到稳定的值；学习率较小时，情况刚好相反。一般而言，我们希望训练初期学习率较大，后期学习率较小，之后会介绍变化学习率的训练方法。")]),_._v(" "),t("p",[_._v("总结整个训练过程，从输入节点 x 开始，通过前向传播得到预测输出 y hat，用 y hat 和 y 得到损失函数值，开始执行反向传播，更新 w 和 b，重复迭代该过程，直到收敛。")]),_._v(" "),t("h2",{attrs:{id:"浅层网络"}},[_._v("浅层网络")]),_._v(" "),t("h3",{attrs:{id:"激活函数"}},[_._v("激活函数")]),_._v(" "),t("p",[_._v("sigmoid：sigmoid 函数常用于二分分类问题，或者多分类问题的最后一层，主要是由于其归一化特性。sigmoid 函数在两侧会出现梯度趋于零的情况，会导致训练缓慢。")]),_._v(" "),t("p",[_._v("tanh：相对于 sigmoid，tanh 函数的优点是梯度值更大，可以使训练速度变快。")]),_._v(" "),t("p",[_._v("ReLU：可以理解为阈值激活（spiking model 的特例，类似生物神经的工作方式），该函数很常用，基本是默认选择的激活函数，优点是不会导致训练缓慢的问题，并且由于激活值为零的节点不会参与反向传播，该函数还有稀疏化网络的效果。")]),_._v(" "),t("p",[_._v("Leaky ReLU：避免了零激活值的结果，使得反向传播过程始终执行，但在实践中很少用。")]),_._v(" "),t("p",[_._v("没有使用非线性激活函数的话，无论多少层的神经网络都等价于单层神经网络（不包含输入层）。")]),_._v(" "),t("h3",{attrs:{id:"如何初始化参数-w、b-的值？"}},[_._v("如何初始化参数 w、b 的值？")]),_._v(" "),t("p",[_._v("随机初始化所有参数，但仅需少量的方差就行，因此使用 Rand（0.01）进行初始化，其中 0.01 也是超参数之一。")]),_._v(" "),t("h2",{attrs:{id:"深度神经网络"}},[t("strong",[_._v("深度神经网络")])]),_._v(" "),t("p",[_._v("CNN 的深度网络可以将底层的简单特征逐层组合成越来越复杂的特征，深度越大，其能分类的图像的复杂度和多样性就越大。RNN 的深度网络也是同样的道理，可以将语音分解为音素，再逐渐组合成字母、单词、句子，执行复杂的语音到文本任务。深度网络的特点是需要大量的训练数据和计算资源，其中涉及大量的矩阵运算，可以在 GPU 上并行执行，还包含了大量的超参数，例如学习率、迭代次数、隐藏层数、激活函数选择、学习率调整方案、批尺寸大小、正则化方法等。")]),_._v(" "),t("h2",{attrs:{id:"偏差与方差"}},[t("strong",[_._v("偏差与方差")])]),_._v(" "),t("p",[_._v("由高偏差带来的欠拟合和由高方差带来的过拟合。一般而言，解决高偏差的问题是选择更复杂的网络或不同的神经网络架构，而解决高方差的问题可以添加正则化、减少模型冗余或使用更多的数据进行训练。")]),_._v(" "),t("h3",{attrs:{id:"正则化"}},[t("strong",[_._v("正则化")])]),_._v(" "),t("p",[_._v("正则化是解决高方差或模型过拟合的主要手段，过去数年，研究者提出和开发了多种适合机器学习算法的正则化方法，如数据增强、L2 正则化（权重衰减）、L1 正则化、Dropout、Drop Connect、随机池化和提前终止等。")])])}],!1,null,null,null);v.default=s.exports}}]);